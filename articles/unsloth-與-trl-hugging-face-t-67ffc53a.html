<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Unsloth 與 TRL / Hugging Face Trainer 深度整合實踐指南（2026）">
  <title>Unsloth 與 TRL / Hugging Face Trainer 深度整合實踐指南（2026） | 知識庫</title>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
  <div class="reading-progress" id="readingProgress"></div>
  <header>
    <div class="container">
      <a href="../" class="logo">知識庫</a>
      <nav>
        <a href="../#buddhism">佛學</a>
        <a href="../#thinking">思維方法</a>
        <a href="../#ai">AI技術</a>
        <a href="../#claude">Claude Code</a>
        <a href="../#game">遊戲</a>
      </nav>
      <button class="theme-toggle" onclick="toggleTheme()" aria-label="切換深色模式">
        <span class="theme-icon">&#9680;</span>
      </button>
    </div>
  </header>

  <main class="container">
    <article>
      <div class="article-header">
        <span class="category-badge category-ai">AI技術</span>
        <h1>Unsloth 與 TRL / Hugging Face Trainer 深度整合實踐指南（2026）</h1>
        <div class="article-meta">
          <span class="date">2026-02-18</span>
          <span class="reading-time">7 分鐘閱讀</span>
          <div class="tags"><span class="tag">Unsloth</span><span class="tag">LLM</span><span class="tag">fine-tuning</span><span class="tag">TRL</span><span class="tag">SFTTrainer</span><span class="tag">DPOTrainer</span><span class="tag">GRPOTrainer</span><span class="tag">Hugging Face</span></div>
        </div>
      </div>

      <details class="article-toc" open>
        <summary>目錄</summary>
        <ol>
          <li><a href="#概述">概述</a></li><li><a href="#一-核心架構-unsloth-trl-整合原理">一、核心架構：Unsloth + TRL 整合原理</a></li>  <li><a href="#1-1-整合方式">1.1 整合方式</a></li>  <li><a href="#1-2-支援的-trl-trainer">1.2 支援的 TRL Trainer</a></li>  <li><a href="#1-3-支援的模型架構-2026-年">1.3 支援的模型架構（2026 年）</a></li><li><a href="#二-sfttrainer-完整整合">二、SFTTrainer 完整整合</a></li>  <li><a href="#2-1-基本配置">2.1 基本配置</a></li>  <li><a href="#2-2-啟用-packing-3x-加速關鍵">2.2 啟用 Packing（3x 加速關鍵）</a></li>  <li><a href="#2-3-預先-tokenize-的資料集">2.3 預先 Tokenize 的資料集</a></li><li><a href="#三-grpotrainer-整合-推理模型訓練">三、GRPOTrainer 整合（推理模型訓練）</a></li>  <li><a href="#3-1-完整配置範例">3.1 完整配置範例</a></li>  <li><a href="#3-2-自定義獎勵函數">3.2 自定義獎勵函數</a></li><li><a href="#四-dpo-orpo-整合">四、DPO / ORPO 整合</a></li>  <li><a href="#4-1-dpotrainer">4.1 DPOTrainer</a></li>  <li><a href="#4-2-dpo-資料集格式">4.2 DPO 資料集格式</a></li><li><a href="#五-效能基準-a100-40gb">五、效能基準（A100 40GB）</a></li><li><a href="#六-最佳實踐與常見陷阱">六、最佳實踐與常見陷阱</a></li>  <li><a href="#6-1-最佳化參數">6.1 最佳化參數</a></li>  <li><a href="#6-2-常見陷阱">6.2 常見陷阱</a></li>  <li><a href="#6-3-模型儲存與匯出">6.3 模型儲存與匯出</a></li><li><a href="#七-與知識庫已有內容的關聯">七、與知識庫已有內容的關聯</a></li><li><a href="#參考來源">參考來源</a></li>
        </ol>
      </details>

      <div class="article-content">
        <h1>Unsloth 與 TRL / Hugging Face Trainer 深度整合實踐指南</h1>
<h2 id="概述">概述</h2>
<p>Unsloth 是一個輕量級 LLM 微調加速函式庫，與 Hugging Face 生態系統（Hub、transformers、PEFT、TRL）完全相容。其核心價值在於透過手動推導反向傳播並重寫為 Triton Kernel，在不犧牲準確度的前提下實現 <strong>2-5x 訓練加速</strong>與 <strong>40-74% 記憶體節省</strong>。本文聚焦 Unsloth 與 TRL 各 Trainer 類別的整合方式，涵蓋 SFTTrainer、DPOTrainer、GRPOTrainer 的完整配置與最佳實踐。</p>
<hr>
<h2 id="一-核心架構-unsloth-trl-整合原理">一、核心架構：Unsloth + TRL 整合原理</h2>
<h3 id="1-1-整合方式">1.1 整合方式</h3>
<p>Unsloth 的整合極為簡潔——只需將 Unsloth 載入的模型傳入 TRL 的 Trainer 類別即可：</p>
<pre><code class="language-python">from unsloth import FastLanguageModel
from trl import SFTTrainer, DPOTrainer, GRPOTrainer

# 載入模型（Unsloth 自動套用 Triton Kernel 優化）
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=&quot;unsloth/Qwen3-8B-bnb-4bit&quot;,
    max_seq_length=2048,
    load_in_4bit=True,
)

# 掛載 LoRA 適配器
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                    &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    lora_alpha=16,
    lora_dropout=0,       # 0 已最佳化
    bias=&quot;none&quot;,           # &quot;none&quot; 已最佳化
    use_gradient_checkpointing=True,
)

# 直接傳入 SFTTrainer / DPOTrainer / GRPOTrainer
trainer = SFTTrainer(model=model, tokenizer=tokenizer, ...)</code></pre>
<h3 id="1-2-支援的-trl-trainer">1.2 支援的 TRL Trainer</h3>
<table>
<tr><th>Trainer</th><th>用途</th><th>Unsloth 加速</th></tr>
<tr><td><code>SFTTrainer</code></td><td>監督微調（Supervised Fine-Tuning）</td><td>2-3x</td></tr>
<tr><td><code>DPOTrainer</code></td><td>直接偏好優化（Direct Preference Optimization）</td><td>1.5-2.7x</td></tr>
<tr><td><code>ORPOTrainer</code></td><td>賠率比偏好優化（Odds Ratio Preference Optimization）</td><td>2x</td></tr>
<tr><td><code>GRPOTrainer</code></td><td>群組相對策略優化（Group Relative Policy Optimization）</td><td>2x</td></tr>
<tr><td><code>PPOTrainer</code></td><td>近端策略優化（Proximal Policy Optimization）</td><td>2x</td></tr>
<tr><td><code>KTOTrainer</code></td><td>知識遷移優化（Knowledge Transfer Optimization）</td><td>2x</td></tr>
</table>
<h3 id="1-3-支援的模型架構-2026-年">1.3 支援的模型架構（2026 年）</h3>
<ul>
<li>Llama 3.x / 3.2 / 3.3</li>
<li>Qwen 2.5 / 3</li>
<li>Mistral / Mixtral</li>
<li>Gemma 2 / 3</li>
<li>Phi-3 / 4</li>
<li>DeepSeek V2 / V3 / R1</li>
<li>Yi / TinyLlama / CodeLlama</li>
</ul>
<hr>
<h2 id="二-sfttrainer-完整整合">二、SFTTrainer 完整整合</h2>
<h3 id="2-1-基本配置">2.1 基本配置</h3>
<pre><code class="language-python">from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
import torch

dataset = load_dataset(&quot;yahma/alpaca-cleaned&quot;, split=&quot;train&quot;)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        dataset_text_field=&quot;text&quot;,
        max_seq_length=2048,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=60,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        output_dir=&quot;outputs&quot;,
        optim=&quot;adamw_8bit&quot;,
        seed=3407,
    ),
)
trainer.train()</code></pre>
<h3 id="2-2-啟用-packing-3x-加速關鍵">2.2 啟用 Packing（3x 加速關鍵）</h3>
<p>Packing 是 Unsloth 2025 年的重大功能更新，透過 padding-free batching 消除填充浪費：</p>
<pre><code class="language-python">args = SFTConfig(
    packing=True,  # 啟用 padding-free packing
    # ... 其他參數同上
)</code></pre>
<p><strong>技術原理</strong>：</p>
<ul>
<li>傳統 batching 將短序列填充至 max_seq_length，浪費大量計算</li>
<li>Packing 模式將多個樣本緊湊打包到同一序列中，用 attention mask 隔離</li>
<li>Unsloth 的 Variable Length RoPE 支援使 packing 時位置編碼正確運作</li>
<li>RoPE kernel 在長上下文時快 2.3x，短上下文時快 1.9x</li>
</ul>
<p><strong>效能提升</strong>（實測資料）：</p>
<ul>
<li>Qwen3-8B：記憶體減少 60%，訓練速度 2x，loss 曲線一致</li>
<li>一般場景：1.1-3x 加速 + 30% 記憶體節省</li>
</ul>
<h3 id="2-3-預先-tokenize-的資料集">2.3 預先 Tokenize 的資料集</h3>
<p>若資料已預先 tokenize，SFTTrainer 會自動跳過內部 tokenization：</p>
<pre><code class="language-python"># 預先準備 input_ids 和 labels
dataset_row = {
    &quot;input_ids&quot;: prompt_tokens + response_tokens,
    &quot;attention_mask&quot;: [1] * len(input_ids),
    &quot;labels&quot;: [-100] * len(prompt_tokens) + response_tokens,
}

# SFTTrainer 自動偵測 is_processed 並跳過 tokenization
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,  # 含 input_ids 的 Dataset
)</code></pre>
<hr>
<h2 id="三-grpotrainer-整合-推理模型訓練">三、GRPOTrainer 整合（推理模型訓練）</h2>
<p>GRPO 是 DeepSeek-R1 使用的強化學習演算法，Unsloth 完美支援：</p>
<h3 id="3-1-完整配置範例">3.1 完整配置範例</h3>
<pre><code class="language-python">from trl import GRPOConfig, GRPOTrainer

# 載入模型時啟用 fast_inference（整合 vLLM）
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=&quot;google/gemma-3-1b-it&quot;,
    max_seq_length=1024,
    load_in_4bit=True,
    fast_inference=True,     # 啟用 vLLM 快速推論
    max_lora_rank=32,
    gpu_memory_utilization=0.6,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                    &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    lora_alpha=32,
    use_gradient_checkpointing=&quot;unsloth&quot;,  # 長上下文微調
    random_state=3407,
)

training_args = GRPOConfig(
    learning_rate=5e-6,
    adam_beta1=0.9,
    adam_beta2=0.99,
    weight_decay=0.1,
    warmup_ratio=0.1,
    lr_scheduler_type=&quot;cosine&quot;,
    optim=&quot;paged_adamw_8bit&quot;,
    logging_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    num_generations=6,         # 每個 prompt 生成的回應數
    max_prompt_length=256,
    max_completion_length=768,
    max_steps=250,
    save_steps=250,
    max_grad_norm=0.1,
    output_dir=&quot;outputs&quot;,
)</code></pre>
<h3 id="3-2-自定義獎勵函數">3.2 自定義獎勵函數</h3>
<pre><code class="language-python">def correctness_reward(prompts, completions, answer, **kwargs):
    &quot;&quot;&quot;正確性獎勵：答案匹配得 2.0 分&quot;&quot;&quot;
    responses = [c[0][&quot;content&quot;] for c in completions]
    extracted = [extract_answer(r) for r in responses]
    return [2.0 if r == a else 0.0 for r, a in zip(extracted, answer)]

def format_reward(completions, **kwargs):
    &quot;&quot;&quot;格式獎勵：符合 XML 結構得 0.5 分&quot;&quot;&quot;
    pattern = r&quot;^\n.*?\n\n\n.*?\n\n$&quot;
    responses = [c[0][&quot;content&quot;] for c in completions]
    return [0.5 if re.match(pattern, r) else 0.0 for r in responses]

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[format_reward, correctness_reward],
    args=training_args,
    train_dataset=dataset,
)
trainer.train()</code></pre>
<hr>
<h2 id="四-dpo-orpo-整合">四、DPO / ORPO 整合</h2>
<h3 id="4-1-dpotrainer">4.1 DPOTrainer</h3>
<pre><code class="language-python">from trl import DPOTrainer, DPOConfig

trainer = DPOTrainer(
    model=model,
    ref_model=None,   # Unsloth 自動處理 reference model
    args=DPOConfig(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_ratio=0.1,
        num_train_epochs=3,
        learning_rate=5e-6,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim=&quot;adamw_8bit&quot;,
        output_dir=&quot;outputs-dpo&quot;,
        beta=0.1,
    ),
    train_dataset=dpo_dataset,
    tokenizer=tokenizer,
)</code></pre>
<h3 id="4-2-dpo-資料集格式">4.2 DPO 資料集格式</h3>
<pre><code class="language-python"># DPO 資料集需要 prompt + chosen + rejected 三欄位
dpo_dataset = {
    &quot;prompt&quot;: &quot;請解釋量子計算&quot;,
    &quot;chosen&quot;: &quot;量子計算利用量子位元的疊加態...&quot;,
    &quot;rejected&quot;: &quot;量子計算就是很快的電腦...&quot;,
}</code></pre>
<hr>
<h2 id="五-效能基準-a100-40gb">五、效能基準（A100 40GB）</h2>
<table>
<tr><th>模型</th><th>資料集</th><th>HF 原生</th><th>HF + Flash Attn 2</th><th>Unsloth</th><th>VRAM 節省</th></tr>
<tr><td>Code Llama 34B</td><td>Slim Orca</td><td>1x</td><td>1.01x</td><td><strong>1.94x</strong></td><td>-22.7%</td></tr>
<tr><td>Llama-2 7B</td><td>Slim Orca</td><td>1x</td><td>0.96x</td><td><strong>1.87x</strong></td><td>-39.3%</td></tr>
<tr><td>Mistral 7B</td><td>Slim Orca</td><td>1x</td><td>1.17x</td><td><strong>1.88x</strong></td><td>-65.9%</td></tr>
<tr><td>Tiny Llama 1.1B</td><td>Alpaca</td><td>1x</td><td>1.55x</td><td><strong>2.74x</strong></td><td>-57.8%</td></tr>
<tr><td>DPO Zephyr</td><td>Ultra Chat</td><td>1x</td><td>1.24x</td><td><strong>1.88x</strong></td><td>-11.6%</td></tr>
</table>
<p><strong>Free Colab T4 基準</strong>：</p>
<ul>
<li>Llama-2 7B：1.95x 加速，-43.3% VRAM</li>
<li>Tiny Llama 1.1B：3.87x 加速，-73.8% VRAM</li>
</ul>
<hr>
<h2 id="六-最佳實踐與常見陷阱">六、最佳實踐與常見陷阱</h2>
<h3 id="6-1-最佳化參數">6.1 最佳化參數</h3>
<pre><code class="language-python"># 推薦的 LoRA 參數
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                    # 一般任務 16；複雜任務 32-64
    lora_alpha=16,           # 通常等於 r
    lora_dropout=0,          # 0 已被 Unsloth 最佳化
    bias=&quot;none&quot;,             # &quot;none&quot; 已被 Unsloth 最佳化
    use_gradient_checkpointing=True,  # 或 &quot;unsloth&quot; 支援長上下文
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                    &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
)</code></pre>
<h3 id="6-2-常見陷阱">6.2 常見陷阱</h3>
<ol>
<li><strong>方法簽名差異</strong>：Unsloth 修改了 SFTTrainer 的方法簽名，新增 <code>num_items_in_batch</code> 等自訂參數。若覆寫 <code>compute_loss</code> 等方法，需處理這些額外參數。</li>
</ol>
<ol>
<li><strong>tokenizer 參數</strong>：標準 SFTTrainer 期望預先 tokenize 的資料，不接受 tokenizer 參數。Unsloth 版本則支援直接傳入 tokenizer。</li>
</ol>
<ol>
<li><strong>版本相容性</strong>：截至 2025 年底，建議使用 <code>unsloth==2025.10.1</code>。版本 2025.10.2 和 2025.10.3 有已知問題。</li>
</ol>
<ol>
<li><strong>記憶體不足處理</strong>：</li>
</ol>
<p>   - 移除 QKVO 中的部分 target_modules
   - 降低 <code>gpu_memory_utilization</code>（GRPO 時）
   - 減少 <code>num_generations</code>（GRPO 時）
   - 使用 <code>use_gradient_checkpointing=&quot;unsloth&quot;</code></p>
<h3 id="6-3-模型儲存與匯出">6.3 模型儲存與匯出</h3>
<pre><code class="language-python"># 儲存 LoRA 權重
model.save_lora(&quot;lora_weights&quot;)

# 合併儲存為 16-bit
model.save_pretrained_merged(&quot;model_16bit&quot;, tokenizer, save_method=&quot;merged_16bit&quot;)

# 匯出 GGUF 格式（供 llama.cpp / Ollama 使用）
model.push_to_hub_gguf(
    &quot;username/model-name&quot;,
    tokenizer,
    quantization_method=[&quot;q4_k_m&quot;, &quot;q8_0&quot;, &quot;q5_k_m&quot;],
    token=&quot;hf_token&quot;,
)</code></pre>
<hr>
<h2 id="七-與知識庫已有內容的關聯">七、與知識庫已有內容的關聯</h2>
<table>
<tr><th>已有筆記</th><th>本文互補點</th></tr>
<tr><td>Unsloth 架構原理與 LoRA/QLoRA</td><td>本文深入 Trainer 整合層（非底層 Kernel）</td></tr>
<tr><td>GGUF 量化匯出與推論部署</td><td>本文涵蓋訓練端完整流程</td></tr>
<tr><td>資料集準備與偏好優化（DPO/ORPO/GRPO）</td><td>本文補充 Trainer API 配置細節</td></tr>
<tr><td>框架比較（Axolotl/LLaMA-Factory）</td><td>本文聚焦 Unsloth 特有的 TRL 整合優勢</td></tr>
<tr><td>繁中 LLM 微調實戰</td><td>本文提供通用 Trainer 配置範本</td></tr>
<tr><td>Pro/Studio 企業版</td><td>本文為開源版 Trainer 使用指南</td></tr>
</table>
<hr>
<h2 id="參考來源">參考來源</h2>
<ul>
<li><a href="https://huggingface.co/blog/unsloth-trl">Make LLM Fine-tuning 2x faster with Unsloth and TRL（HuggingFace Blog）</a></li>
<li><a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide">Unsloth 官方文件 — Fine-tuning Guide</a></li>
<li><a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/reinforcement-learning-dpo-orpo-and-kto">Unsloth 官方文件 — Preference Optimization</a></li>
<li><a href="https://docs.unsloth.ai/new/3x-faster-training-packing">3x Faster Training with Packing（Unsloth Blog）</a></li>
<li><a href="https://huggingface.co/learn/llm-course/en/chapter12/6">Practical Exercise: GRPO with Unsloth（HuggingFace LLM Course）</a></li>
<li><a href="https://microsoft.github.io/agent-lightning/latest/how-to/unsloth-sft/">SFT with Unsloth — Agent-lightning（Microsoft）</a></li>
<li><a href="https://github.com/unslothai/unsloth">Unsloth GitHub</a></li>
</ul>
<hr>
<p><em>研究日期：2026-02-18</em>
<em>研究者：Claude Code Agent</em>
<em>任務類型：unsloth_research（Unsloth TRL Trainer 整合）</em></p>

      </div>

      <nav class="article-nav"><a href="ai-驅動的智慧醫療與公共衛生監測系統技術架構全球實踐與-607236f5.html" class="nav-prev"><span class="nav-label">&larr; 上一篇</span><span class="nav-title">AI 驅動的智慧醫療與公共衛生監測系統：技術架構、全球實踐與台灣機會</span></a><a href="github-scout-agent-架構與-skill--5fde6d02.html" class="nav-next"><span class="nav-label">下一篇 &rarr;</span><span class="nav-title">GitHub Scout: Agent 架構與 Skill 系統 - 2026-02-18</span></a></nav>

      <a href="../" class="back-link">&larr; 返回首頁</a>
    </article>
  </main>

  <button class="back-to-top" id="backToTop" onclick="window.scrollTo({top:0,behavior:'smooth'})" aria-label="回到頂部">&uarr;</button>

  <footer>
    <div class="container">
      <p>Powered by RAG Knowledge Base</p>
    </div>
  </footer>
  <script>
    function toggleTheme(){const b=document.body;b.classList.toggle('dark');localStorage.setItem('theme',b.classList.contains('dark')?'dark':'light')}
    if(localStorage.getItem('theme')==='dark')document.body.classList.add('dark');
    window.addEventListener('scroll',function(){
      const prog=document.getElementById('readingProgress');
      const btn=document.getElementById('backToTop');
      const h=document.documentElement.scrollHeight-window.innerHeight;
      const pct=h>0?(window.scrollY/h)*100:0;
      prog.style.width=pct+'%';
      btn.classList.toggle('visible',window.scrollY>300);
    });
  </script>
</body>
</html>
