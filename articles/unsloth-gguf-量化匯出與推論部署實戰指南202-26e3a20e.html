<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Unsloth GGUF 量化匯出與推論部署實戰指南（2026）">
  <title>Unsloth GGUF 量化匯出與推論部署實戰指南（2026） | 知識庫</title>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
  <div class="reading-progress" id="readingProgress"></div>
  <header>
    <div class="container">
      <a href="../" class="logo">知識庫</a>
      <nav>
        <a href="../#buddhism">佛學</a>
        <a href="../#thinking">思維方法</a>
        <a href="../#ai">AI技術</a>
        <a href="../#claude">Claude Code</a>
        <a href="../#game">遊戲</a>
      </nav>
      <button class="theme-toggle" onclick="toggleTheme()" aria-label="切換深色模式">
        <span class="theme-icon">&#9680;</span>
      </button>
    </div>
  </header>

  <main class="container">
    <article>
      <div class="article-header">
        <span class="category-badge category-ai">AI技術</span>
        <h1>Unsloth GGUF 量化匯出與推論部署實戰指南（2026）</h1>
        <div class="article-meta">
          <span class="date">2026-02-16</span>
          <div class="tags"><span class="tag">Unsloth</span><span class="tag">GGUF</span><span class="tag">量化</span><span class="tag">LLM</span><span class="tag">fine-tuning</span><span class="tag">llama.cpp</span><span class="tag">Ollama</span><span class="tag">vLLM</span></div>
        </div>
      </div>

      <details class="article-toc" open>
        <summary>目錄</summary>
        <ol>
          <li><a href="#概述">概述</a></li><li><a href="#gguf-量化方法完整列表">GGUF 量化方法完整列表</a></li>  <li><a href="#推薦方法-快捷方式">推薦方法（快捷方式）</a></li>  <li><a href="#精度保留方法">精度保留方法</a></li>  <li><a href="#k-quant-系列-最常用">K-Quant 系列（最常用）</a></li>  <li><a href="#傳統量化方法">傳統量化方法</a></li>  <li><a href="#i-quant-系列-極端壓縮">I-Quant 系列（極端壓縮）</a></li><li><a href="#核心匯出操作">核心匯出操作</a></li>  <li><a href="#方法一-一鍵-gguf-匯出-推薦">方法一：一鍵 GGUF 匯出（推薦）</a></li>  <li><a href="#方法二-推送到-hugging-face-hub">方法二：推送到 Hugging Face Hub</a></li>  <li><a href="#方法三-先合併再手動轉換">方法三：先合併再手動轉換</a></li><li><a href="#部署到各推論引擎">部署到各推論引擎</a></li>  <li><a href="#1-ollama-部署">1. Ollama 部署</a></li>  <li><a href="#2-llama-server-openai-相容-api">2. llama-server（OpenAI 相容 API）</a></li>  <li><a href="#3-vllm-部署">3. vLLM 部署</a></li>  <li><a href="#4-lm-studio">4. LM Studio</a></li><li><a href="#dynamic-2-0-量化-unsloth-獨家">Dynamic 2.0 量化（Unsloth 獨家）</a></li>  <li><a href="#核心特性">核心特性</a></li>  <li><a href="#基準測試結果">基準測試結果</a></li>  <li><a href="#使用-dynamic-2-0-gguf">使用 Dynamic 2.0 GGUF</a></li><li><a href="#量化方法選擇指南">量化方法選擇指南</a></li>  <li><a href="#7b-模型檔案大小參考">7B 模型檔案大小參考</a></li><li><a href="#常見問題與解法">常見問題與解法</a></li>  <li><a href="#1-匯出後推論產生亂碼或無限迴圈">1. 匯出後推論產生亂碼或無限迴圈</a></li>  <li><a href="#2-儲存時-gpu-oom">2. 儲存時 GPU OOM</a></li>  <li><a href="#3-sentencepiece-模型的-gguf-相容性">3. Sentencepiece 模型的 GGUF 相容性</a></li>  <li><a href="#4-windows-環境手動編譯-llama-cpp">4. Windows 環境手動編譯 llama.cpp</a></li>  <li><a href="#5-量化精度驗證">5. 量化精度驗證</a></li><li><a href="#儲存方法總覽">儲存方法總覽</a></li><li><a href="#參考來源">參考來源</a></li>
        </ol>
      </details>

      <div class="article-content">
        <h1>Unsloth GGUF 量化匯出與推論部署實戰指南</h1>
<h2 id="概述">概述</h2>
<p>Unsloth 微調完成後，模型需經過量化匯出才能部署到各種推論引擎。GGUF（GPT-Generated Unified Format）是 llama.cpp 團隊設計的模型格式，已成為本地 LLM 推論的事實標準。Unsloth 內建一站式 GGUF 匯出功能，透過 <code>save_pretrained_gguf()</code> 方法可直接將微調模型量化並匯出為 GGUF 格式，無需手動操作 llama.cpp 工具鏈。支援 20+ 種量化方法，匯出的模型可直接在 Ollama、LM Studio、Open WebUI、llama-server 等推論引擎上運行。2025 年 4 月推出的 Dynamic 2.0 量化技術更進一步提升了量化模型的精度，在 5-shot MMLU 基準測試中創下新標竿。</p>
<hr>
<h2 id="gguf-量化方法完整列表">GGUF 量化方法完整列表</h2>
<p>Unsloth 支援以下量化方法（透過 <code>quantization_method</code> 參數指定）：</p>
<h3 id="推薦方法-快捷方式">推薦方法（快捷方式）</h3>
<table>
<tr><th>方法</th><th>說明</th></tr>
<tr><td><code>not_quantized</code></td><td>快速轉換，不量化。推論慢、檔案大</td></tr>
<tr><td><code>fast_quantized</code></td><td>快速轉換，推論與檔案大小適中</td></tr>
<tr><td><code>quantized</code></td><td>慢速轉換，推論快、檔案小</td></tr>
</table>
<h3 id="精度保留方法">精度保留方法</h3>
<table>
<tr><th>方法</th><th>說明</th></tr>
<tr><td><code>f32</code></td><td>保留 100% 精度，但超慢且耗記憶體</td></tr>
<tr><td><code>bf16</code></td><td>Bfloat16，最快轉換 + 100% 精度</td></tr>
<tr><td><code>f16</code></td><td>Float16，最快轉換 + 100% 精度</td></tr>
</table>
<h3 id="k-quant-系列-最常用">K-Quant 系列（最常用）</h3>
<table>
<tr><th>方法</th><th>bits/weight</th><th>說明</th></tr>
<tr><td><code>q8_0</code></td><td>8-bit</td><td>快速轉換，資源用量高但品質可接受</td></tr>
<tr><td><code>q6_k</code></td><td>6-bit</td><td>使用 Q8_K 量化所有張量</td></tr>
<tr><td><code>q5_k_m</code></td><td>5-bit</td><td><strong>推薦</strong>。attention.wv/feed_forward.w2 用 Q6_K，其餘 Q5_K</td></tr>
<tr><td><code>q5_k_s</code></td><td>5-bit</td><td>所有張量使用 Q5_K</td></tr>
<tr><td><code>q4_k_m</code></td><td>4-bit</td><td><strong>推薦</strong>。attention.wv/feed_forward.w2 用 Q6_K，其餘 Q4_K</td></tr>
<tr><td><code>q4_k_s</code></td><td>4-bit</td><td>所有張量使用 Q4_K</td></tr>
<tr><td><code>q3_k_l</code></td><td>3-bit</td><td>attention.wv/wo/feed_forward.w2 用 Q5_K，其餘 Q3_K</td></tr>
<tr><td><code>q3_k_m</code></td><td>3-bit</td><td>attention.wv/wo/feed_forward.w2 用 Q4_K，其餘 Q3_K</td></tr>
<tr><td><code>q3_k_s</code></td><td>3-bit</td><td>所有張量使用 Q3_K</td></tr>
<tr><td><code>q3_k_xs</code></td><td>3-bit</td><td>超小型 3-bit 量化</td></tr>
<tr><td><code>q2_k</code></td><td>2-bit</td><td>attention.vw/feed_forward.w2 用 Q4_K，其餘 Q2_K</td></tr>
</table>
<h3 id="傳統量化方法">傳統量化方法</h3>
<table>
<tr><th>方法</th><th>說明</th></tr>
<tr><td><code>q4_0</code></td><td>原始 4-bit 量化</td></tr>
<tr><td><code>q4_1</code></td><td>比 q4_0 精度高，推論比 q5 快</td></tr>
<tr><td><code>q5_0</code></td><td>精度更高，資源更多</td></tr>
<tr><td><code>q5_1</code></td><td>精度最高的 5-bit</td></tr>
</table>
<h3 id="i-quant-系列-極端壓縮">I-Quant 系列（極端壓縮）</h3>
<table>
<tr><th>方法</th><th>bpw</th><th>說明</th></tr>
<tr><td><code>iq2_xxs</code></td><td>2.06</td><td>超低位元量化</td></tr>
<tr><td><code>iq2_xs</code></td><td>2.31</td><td>低位元量化</td></tr>
<tr><td><code>iq3_xxs</code></td><td>3.06</td><td>低位元量化</td></tr>
</table>
<hr>
<h2 id="核心匯出操作">核心匯出操作</h2>
<h3 id="方法一-一鍵-gguf-匯出-推薦">方法一：一鍵 GGUF 匯出（推薦）</h3>
<pre><code class="language-python">from unsloth import FastLanguageModel

# 載入微調後的模型
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=&quot;./my-finetuned-lora&quot;,
    max_seq_length=2048,
    load_in_4bit=True,
)

# 匯出為 GGUF（本地儲存）
# q4_k_m：最佳平衡點（精度 vs 檔案大小 vs 推論速度）
model.save_pretrained_gguf(
    &quot;my-model-gguf&quot;,       # 輸出目錄
    tokenizer,
    quantization_method=&quot;q4_k_m&quot;  # 量化方法
)

# 其他常用量化方法
model.save_pretrained_gguf(&quot;dir&quot;, tokenizer, quantization_method=&quot;q8_0&quot;)   # 高品質
model.save_pretrained_gguf(&quot;dir&quot;, tokenizer, quantization_method=&quot;q5_k_m&quot;) # 品質/大小平衡
model.save_pretrained_gguf(&quot;dir&quot;, tokenizer, quantization_method=&quot;f16&quot;)    # 完整精度</code></pre>
<h3 id="方法二-推送到-hugging-face-hub">方法二：推送到 Hugging Face Hub</h3>
<pre><code class="language-python"># 直接推送 GGUF 到 HF Hub
model.push_to_hub_gguf(
    &quot;your-username/my-model-gguf&quot;,
    tokenizer,
    quantization_method=&quot;q4_k_m&quot;,
    token=&quot;hf_your_token&quot;  # HF API token
)</code></pre>
<h3 id="方法三-先合併再手動轉換">方法三：先合併再手動轉換</h3>
<pre><code class="language-python"># 步驟 1：儲存合併後的 16-bit 模型
model.save_pretrained_merged(
    &quot;merged_model&quot;,
    tokenizer,
    save_method=&quot;merged_16bit&quot;
)</code></pre>
<pre><code class="language-bash"># 步驟 2：安裝 llama.cpp
apt-get update
apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y
git clone https://github.com/ggerganov/llama.cpp
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON
cmake --build llama.cpp/build --config Release -j \
    --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli
cp llama.cpp/build/bin/llama-* llama.cpp

# 步驟 3：轉換為 GGUF
python llama.cpp/convert_hf_to_gguf.py merged_model --outfile model.gguf --outtype f16

# 步驟 4：量化（可選）
./llama.cpp/llama-quantize model.gguf model-q4km.gguf q4_k_m</code></pre>
<hr>
<h2 id="部署到各推論引擎">部署到各推論引擎</h2>
<h3 id="1-ollama-部署">1. Ollama 部署</h3>
<p>Unsloth 自動生成 Ollama 所需的 Modelfile（含正確的 chat template）。</p>
<pre><code class="language-python"># 匯出 GGUF（Unsloth 自動生成 Modelfile）
model.save_pretrained_gguf(
    &quot;my-model&quot;,
    tokenizer,
    quantization_method=&quot;q8_0&quot;
)</code></pre>
<pre><code class="language-bash"># 啟動 Ollama 服務
ollama serve &amp;

# 用自動生成的 Modelfile 建立模型
ollama create my-model -f ./my-model/Modelfile

# 執行推論
ollama run my-model &quot;你好，請介紹你自己&quot;</code></pre>
<p><strong>關鍵注意</strong>：Unsloth 的 <code>OLLAMA_TEMPLATES</code> 字典自動映射模型架構到正確的 Ollama chat template，避免推論時產生亂碼。</p>
<h3 id="2-llama-server-openai-相容-api">2. llama-server（OpenAI 相容 API）</h3>
<pre><code class="language-bash"># 啟動 OpenAI 相容伺服器
./llama.cpp/llama-server \
    -m model-q4km.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    -ngl 99  # GPU 層數

# 使用 curl 呼叫
curl http://localhost:8080/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#x27;{&quot;model&quot;:&quot;model&quot;,&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;Hello!&quot;}]}&#x27;</code></pre>
<h3 id="3-vllm-部署">3. vLLM 部署</h3>
<pre><code class="language-python"># 先儲存為 16-bit 合併模型
model.save_pretrained_merged(
    &quot;my-model-vllm&quot;,
    tokenizer,
    save_method=&quot;merged_16bit&quot;
)</code></pre>
<pre><code class="language-python"># vLLM 推論
from vllm import LLM, SamplingParams

llm = LLM(model=&quot;my-model-vllm&quot;)
sampling_params = SamplingParams(temperature=0.7, max_tokens=512)
outputs = llm.generate([&quot;Tell me about AI&quot;], sampling_params)
print(outputs[0].outputs[0].text)</code></pre>
<h3 id="4-lm-studio">4. LM Studio</h3>
<ol>
<li>將 GGUF 檔案放入 LM Studio 的模型目錄</li>
<li>在 LM Studio UI 中選擇模型</li>
<li>確保選用正確的 chat template（與訓練時一致）</li>
</ol>
<hr>
<h2 id="dynamic-2-0-量化-unsloth-獨家">Dynamic 2.0 量化（Unsloth 獨家）</h2>
<p>2025 年 4 月推出的 Dynamic 2.0 是 Unsloth 的進階量化技術：</p>
<h3 id="核心特性">核心特性</h3>
<ul>
<li><strong>智慧逐層量化</strong>：動態調整每一層的量化類型，不同模型使用不同的量化方案</li>
<li><strong>模型專屬方案</strong>：Gemma 3 的量化策略與 Llama 4 完全不同</li>
<li><strong>高品質校準資料集</strong>：30 萬到 150 萬 tokens 的手工精選數據，針對對話場景優化</li>
<li><strong>通用架構支援</strong>：從 MoE（如 DeepSeek-R1）到 Dense 模型（如 Gemma 3）均可使用</li>
</ul>
<h3 id="基準測試結果">基準測試結果</h3>
<table>
<tr><th>模型</th><th>方法</th><th>5-shot MMLU</th></tr>
<tr><td>Gemma 3 12B (bf16)</td><td>未量化</td><td>67.15%</td></tr>
<tr><td>Gemma 3 12B</td><td>Google QAT Q4_0</td><td>67.07%</td></tr>
<tr><td>Gemma 3 12B</td><td>Dynamic 2.0 Q4_K_M</td><td>超越標準 imatrix</td></tr>
<tr><td>Llama 4 Scout</td><td>Dynamic 2.0</td><td>超越傳統量化方法</td></tr>
</table>
<h3 id="使用-dynamic-2-0-gguf">使用 Dynamic 2.0 GGUF</h3>
<p>Dynamic 2.0 模型可從 Hugging Face 直接下載：</p>
<ul>
<li><a href="https://huggingface.co/collections/unsloth/">Unsloth Dynamic 2.0 合集</a></li>
<li>支援 DeepSeek-R1、DeepSeek-V3、Gemma 3（12B/27B）、Llama 4 Scout</li>
</ul>
<hr>
<h2 id="量化方法選擇指南">量化方法選擇指南</h2>
<table>
<tr><th>場景</th><th>推薦方法</th><th>理由</th></tr>
<tr><td>生產部署（品質優先）</td><td><code>q8_0</code></td><td>接近原始精度，推論可接受</td></tr>
<tr><td>一般使用（平衡）</td><td><code>q5_k_m</code></td><td>精度與大小的最佳平衡</td></tr>
<tr><td>本地推論（預設推薦）</td><td><code>q4_k_m</code></td><td>關鍵層用 Q6_K 保精度，其餘 Q4_K 省空間</td></tr>
<tr><td>行動裝置 / 嵌入式</td><td><code>q3_k_m</code> 或 <code>q2_k</code></td><td>極致壓縮，精度損失較大</td></tr>
<tr><td>開發測試</td><td><code>f16</code></td><td>完整精度，方便除錯</td></tr>
<tr><td>存檔備份</td><td><code>not_quantized</code> 或 <code>f16</code></td><td>保留完整品質</td></tr>
</table>
<h3 id="7b-模型檔案大小參考">7B 模型檔案大小參考</h3>
<table>
<tr><th>量化方法</th><th>約略大小</th><th>精度損失</th></tr>
<tr><td>f16</td><td>~14 GB</td><td>0%</td></tr>
<tr><td>q8_0</td><td>~7 GB</td><td>極小</td></tr>
<tr><td>q5_k_m</td><td>~5 GB</td><td>小</td></tr>
<tr><td>q4_k_m</td><td>~4 GB</td><td>小到中</td></tr>
<tr><td>q3_k_m</td><td>~3 GB</td><td>中</td></tr>
<tr><td>q2_k</td><td>~2.5 GB</td><td>大</td></tr>
</table>
<hr>
<h2 id="常見問題與解法">常見問題與解法</h2>
<h3 id="1-匯出後推論產生亂碼或無限迴圈">1. 匯出後推論產生亂碼或無限迴圈</h3>
<p><strong>根因</strong>：Chat template 不一致。訓練時的 chat template 必須與推論時完全相同。</p>
<p><strong>解法</strong>：</p>
<ul>
<li>使用 Unsloth 的 conversational notebook（自動設定正確 template）</li>
<li>確認 EOS token 正確（不同模型的 EOS token 不同）</li>
<li>檢查推論引擎是否多加了 BOS token</li>
</ul>
<h3 id="2-儲存時-gpu-oom">2. 儲存時 GPU OOM</h3>
<p><strong>解法</strong>：降低 <code>maximum_memory_usage</code> 參數：</p>
<pre><code class="language-python">model.save_pretrained_gguf(
    &quot;dir&quot;, tokenizer,
    quantization_method=&quot;q4_k_m&quot;,
    maximum_memory_usage=0.5  # 預設 0.75，降到 0.5
)</code></pre>
<h3 id="3-sentencepiece-模型的-gguf-相容性">3. Sentencepiece 模型的 GGUF 相容性</h3>
<p>Unsloth 內建 <code>fix_sentencepiece_gguf</code> 函數自動處理 Sentencepiece tokenizer 的 GGUF 相容性問題。</p>
<h3 id="4-windows-環境手動編譯-llama-cpp">4. Windows 環境手動編譯 llama.cpp</h3>
<pre><code class="language-bash"># Windows (MSVC + CUDA)
cmake llama.cpp -B llama.cpp/build -DGGML_CUDA=ON
cmake --build llama.cpp/build --config Release</code></pre>
<h3 id="5-量化精度驗證">5. 量化精度驗證</h3>
<p>建議匯出後用相同 prompt 在原始模型和量化模型上做 A/B 測試，確認回答品質可接受。</p>
<hr>
<h2 id="儲存方法總覽">儲存方法總覽</h2>
<table>
<tr><th>方法</th><th>用途</th><th>程式碼</th></tr>
<tr><td>LoRA adapter</td><td>小檔案（~100MB），需基礎模型</td><td><code>save_method=&quot;lora&quot;</code></td></tr>
<tr><td>合併 16-bit</td><td>vLLM / HF 推論</td><td><code>save_method=&quot;merged_16bit&quot;</code></td></tr>
<tr><td>合併 4-bit</td><td>DPO 訓練 / HF 線上推論</td><td><code>save_method=&quot;merged_4bit&quot;</code></td></tr>
<tr><td>GGUF</td><td>Ollama / llama.cpp / LM Studio</td><td><code>save_pretrained_gguf()</code></td></tr>
<tr><td>HF Hub GGUF</td><td>分享量化模型</td><td><code>push_to_hub_gguf()</code></td></tr>
</table>
<hr>
<h2 id="參考來源">參考來源</h2>
<ul>
<li><a href="https://docs.unsloth.ai/basics/saving-models/saving-to-gguf">Unsloth 官方文件 - Saving to GGUF</a></li>
<li><a href="https://docs.unsloth.ai/basics/inference-and-deployment">Unsloth 官方文件 - Inference &amp; Deployment</a></li>
<li><a href="https://docs.unsloth.ai/basics/saving-models/saving-to-ollama">Unsloth 官方文件 - Saving to Ollama</a></li>
<li><a href="https://github.com/unslothai/unsloth/blob/main/unsloth/save.py">Unsloth GitHub - save.py 原始碼</a></li>
<li><a href="https://unsloth.ai/blog/dynamic-v2">Unsloth Dynamic 2.0 GGUFs 部落格</a></li>
<li><a href="https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf">Unsloth GitHub Wiki - Manually Saving to GGUF</a></li>
<li><a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp">llama.cpp GGUF 量化定義</a></li>
</ul>
<p><em>研究日期：2026-02-16</em>
<em>研究者：Unsloth Research Agent</em></p>

      </div>

      <a href="../" class="back-link">&larr; 返回首頁</a>
    </article>
  </main>

  <button class="back-to-top" id="backToTop" onclick="window.scrollTo({top:0,behavior:'smooth'})" aria-label="回到頂部">&uarr;</button>

  <footer>
    <div class="container">
      <p>Powered by RAG Knowledge Base</p>
    </div>
  </footer>
  <script>
    function toggleTheme(){const b=document.body;b.classList.toggle('dark');localStorage.setItem('theme',b.classList.contains('dark')?'dark':'light')}
    if(localStorage.getItem('theme')==='dark')document.body.classList.add('dark');
    window.addEventListener('scroll',function(){
      const prog=document.getElementById('readingProgress');
      const btn=document.getElementById('backToTop');
      const h=document.documentElement.scrollHeight-window.innerHeight;
      const pct=h>0?(window.scrollY/h)*100:0;
      prog.style.width=pct+'%';
      btn.classList.toggle('visible',window.scrollY>300);
    });
  </script>
</body>
</html>
