<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Open R1（Hugging Face）：完全開源複現 DeepSeek-R1 推理模型的里程碑專案（25.9K Stars）">
  <title>Open R1（Hugging Face）：完全開源複現 DeepSeek-R1 推理模型的里程碑專案（25.9K Stars） | 知識庫</title>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
  <div class="reading-progress" id="readingProgress"></div>
  <header>
    <div class="container">
      <a href="../" class="logo">知識庫</a>
      <nav>
        <a href="../#buddhism">佛學</a>
        <a href="../#thinking">思維方法</a>
        <a href="../#ai">AI技術</a>
        <a href="../#claude">Claude Code</a>
        <a href="../#game">遊戲</a>
      </nav>
      <button class="theme-toggle" onclick="toggleTheme()" aria-label="切換深色模式">
        <span class="theme-icon">&#9680;</span>
      </button>
    </div>
  </header>

  <main class="container">
    <article>
      <div class="article-header">
        <span class="category-badge category-ai">AI技術</span>
        <h1>Open R1（Hugging Face）：完全開源複現 DeepSeek-R1 推理模型的里程碑專案（25.9K Stars）</h1>
        <div class="article-meta">
          <span class="date">2026-02-16</span>
          <div class="tags"><span class="tag">GitHub</span><span class="tag">Open R1</span><span class="tag">Hugging Face</span><span class="tag">DeepSeek-R1</span><span class="tag">GRPO</span><span class="tag">推理模型</span><span class="tag">強化學習</span><span class="tag">開源專案</span></div>
        </div>
      </div>

      <details class="article-toc" open>
        <summary>目錄</summary>
        <ol>
          <li><a href="#概述">概述</a></li><li><a href="#為何重要">為何重要</a></li>  <li><a href="#deepseek-r1-的成就">DeepSeek-R1 的成就</a></li>  <li><a href="#缺失的關鍵拼圖">缺失的關鍵拼圖</a></li><li><a href="#三步驟複現計畫">三步驟複現計畫</a></li>  <li><a href="#步驟-1-蒸餾-distillation-已完成">步驟 1：蒸餾（Distillation）— 已完成</a></li>  <li><a href="#步驟-2-純-rl-管線複現-進行中">步驟 2：純 RL 管線複現（進行中）</a></li>  <li><a href="#步驟-3-多階段訓練-規劃中">步驟 3：多階段訓練（規劃中）</a></li><li><a href="#核心技術元件">核心技術元件</a></li>  <li><a href="#grpo-group-relative-policy-optimization">GRPO（Group Relative Policy Optimization）</a></li>  <li><a href="#獎勵函數系統">獎勵函數系統</a></li>  <li><a href="#資料集生態">資料集生態</a></li>  <li><a href="#基礎設施">基礎設施</a></li><li><a href="#deepseek-r1-訓練方法解析">DeepSeek-R1 訓練方法解析</a></li>  <li><a href="#r1-zero-純-rl-路線">R1-Zero（純 RL 路線）</a></li>  <li><a href="#r1-完整訓練路線">R1（完整訓練路線）</a></li>  <li><a href="#底層模型-deepseek-v3">底層模型：DeepSeek-V3</a></li><li><a href="#社群影響與戰略意義">社群影響與戰略意義</a></li>  <li><a href="#grpo-對-ai-研究的深遠影響">GRPO 對 AI 研究的深遠影響</a></li><li><a href="#對-daily-digest-prompt-專案的啟發">對 Daily-Digest-Prompt 專案的啟發</a></li><li><a href="#參考來源">參考來源</a></li>
        </ol>
      </details>

      <div class="article-content">
        <h1>Open R1：完全開源複現 DeepSeek-R1 推理模型</h1>
<h2 id="概述">概述</h2>
<p>Open R1 是 Hugging Face 於 2025 年 1 月發起的開源計畫，目標是<strong>完全複現 DeepSeek-R1 的資料集與訓練管線</strong>。DeepSeek-R1 雖然釋出了模型權重，但未公開訓練資料集和訓練程式碼，Open R1 填補了這一空白，讓全球開發者都能複現和改進推理模型。</p>
<ul>
<li><strong>GitHub Stars</strong>：25,900+（發布 3 天內突破 10,000 星）</li>
<li><strong>Forks</strong>：2,400+</li>
<li><strong>授權</strong>：Apache-2.0</li>
<li><strong>技術棧</strong>：Python 3.11、PyTorch 2.6.0、vLLM 0.8.5+、DeepSpeed ZeRO-2/3、CUDA 12.4</li>
</ul>
<hr>
<h2 id="為何重要">為何重要</h2>
<h3 id="deepseek-r1-的成就">DeepSeek-R1 的成就</h3>
<p>DeepSeek-R1 證明了語言模型可以透過<strong>推理時增加計算量</strong>顯著提升推理能力。核心洞見：LLM 可以被訓練為「更長時間思考」問題，類似人類的推理過程。</p>
<ul>
<li>效能與 OpenAI o1 相當甚至更好</li>
<li>使用<strong>純強化學習</strong>（無人類監督）教會模型推理</li>
<li>AIME 2024 達 79.8%，MATH-500 達 97.3%，GPQA Diamond 達 71.5%</li>
</ul>
<h3 id="缺失的關鍵拼圖">缺失的關鍵拼圖</h3>
<p>DeepSeek 未提供：</p>
<ol>
<li>資料收集方法論</li>
<li>訓練程式碼與超參數配置</li>
<li>擴展法則（Scaling Laws）的計算與資料權衡</li>
</ol>
<hr>
<h2 id="三步驟複現計畫">三步驟複現計畫</h2>
<h3 id="步驟-1-蒸餾-distillation-已完成">步驟 1：蒸餾（Distillation）— 已完成</h3>
<p><strong>目標</strong>：從 DeepSeek-R1 蒸餾高品質推理資料集</p>
<ul>
<li>2025 年 2 月：發布 <strong>OpenR1-Math-220k</strong> 資料集，模型匹配 DeepSeek 蒸餾效能</li>
<li>2025 年 3 月：發布 <strong>CodeForces-CoTs</strong>（10K 競程題目 + 100K 解答），7B 模型超越 Claude 3.7 Sonnet</li>
<li>2025 年 5 月：發布 <strong>Mixture-of-Thoughts</strong>（350K 驗證推理軌跡，涵蓋數學/程式/科學）+ <strong>OpenR1-Distill-7B</strong></li>
</ul>
<p><strong>OpenR1-Distill-7B 基準測試</strong>：</p>
<table>
<tr><th>基準</th><th>分數</th></tr>
<tr><td>AIME 2024</td><td>52.7%</td></tr>
<tr><td>MATH-500</td><td>89.0%</td></tr>
<tr><td>GPQA Diamond</td><td>52.8%</td></tr>
</table>
<h3 id="步驟-2-純-rl-管線複現-進行中">步驟 2：純 RL 管線複現（進行中）</h3>
<p><strong>目標</strong>：複現 R1-Zero 的訓練方法</p>
<ul>
<li>策劃大規模資料集（數學、推理、程式碼）</li>
<li>實作 GRPO（Group Relative Policy Optimization）</li>
<li>開發可驗證獎勵與偏好獎勵機制</li>
</ul>
<h3 id="步驟-3-多階段訓練-規劃中">步驟 3：多階段訓練（規劃中）</h3>
<p><strong>目標</strong>：展示從基礎模型到 RL 微調的完整管線</p>
<ul>
<li>SFT（監督微調）→ RL（強化學習）→ 精煉</li>
</ul>
<hr>
<h2 id="核心技術元件">核心技術元件</h2>
<h3 id="grpo-group-relative-policy-optimization">GRPO（Group Relative Policy Optimization）</h3>
<p>GRPO 是 DeepSeek 提出的 RL 框架，在 Open R1 中完整實作：</p>
<ul>
<li><strong>概念</strong>：對同一提示生成一組回答，以組內相對表現作為優勢估計，取代傳統 PPO 中的 Critic 模型</li>
<li><strong>優勢</strong>：無需額外的價值函數網路，降低計算成本</li>
<li><strong>訓練模式</strong>：支援單節點 colocate 模式與多節點 vLLM 伺服器分離模式</li>
</ul>
<h3 id="獎勵函數系統">獎勵函數系統</h3>
<p><strong>可驗證獎勵</strong>：</p>
<ul>
<li><strong>E2B</strong>：雲端 Python 沙箱執行</li>
<li><strong>Morph</strong>：多語言支援（Python、C++、Java、Rust）</li>
<li><strong>Piston</strong>：自建執行引擎</li>
</ul>
<p><strong>偏好獎勵</strong>：基於品質與清晰度的人類偏好獎勵</p>
<h3 id="資料集生態">資料集生態</h3>
<table>
<tr><th>資料集</th><th>規模</th><th>領域</th></tr>
<tr><td>Mixture-of-Thoughts</td><td>350K 推理軌跡</td><td>數學、程式、科學</td></tr>
<tr><td>OpenR1-Math-220k</td><td>220K</td><td>數學推理</td></tr>
<tr><td>CodeForces-CoTs</td><td>100K</td><td>競程解題</td></tr>
</table>
<h3 id="基礎設施">基礎設施</h3>
<ul>
<li><strong>分散式訓練</strong>：Accelerate + DDP/DeepSpeed ZeRO-2/3</li>
<li><strong>推論引擎</strong>：vLLM 0.8.5+（訓練期間快速推論）</li>
<li><strong>叢集管理</strong>：SLURM 多節點作業排程</li>
<li><strong>硬體需求</strong>：測試於 8×H100（80GB）節點</li>
<li><strong>實驗追蹤</strong>：WandB 整合</li>
</ul>
<hr>
<h2 id="deepseek-r1-訓練方法解析">DeepSeek-R1 訓練方法解析</h2>
<h3 id="r1-zero-純-rl-路線">R1-Zero（純 RL 路線）</h3>
<ul>
<li>無監督微調，直接使用強化學習</li>
<li>簡單獎勵：答案正確性 + 格式結構</li>
<li>模型自發展現：問題分解、自我驗證、重新評估</li>
<li><strong>限制</strong>：輸出缺乏清晰度和可讀性</li>
</ul>
<h3 id="r1-完整訓練路線">R1（完整訓練路線）</h3>
<ol>
<li><strong>冷啟動階段</strong>：在少量高品質範例上微調</li>
<li><strong>多輪 RL 精煉</strong>：結合偏好獎勵與可驗證獎勵</li>
<li><strong>拒絕低品質輸出</strong>：過濾不良推理軌跡</li>
<li><strong>結果</strong>：推理能力 + 表達清晰度兼具</li>
</ol>
<h3 id="底層模型-deepseek-v3">底層模型：DeepSeek-V3</h3>
<ul>
<li><strong>規模</strong>：671B 參數 MoE（混合專家）模型</li>
<li><strong>訓練成本</strong>：僅 550 萬美元（架構創新大幅降低成本）</li>
<li><strong>關鍵創新</strong>：Multi-Token Prediction (MTP)、Multi-Head Latent Attention (MLA)</li>
</ul>
<hr>
<h2 id="社群影響與戰略意義">社群影響與戰略意義</h2>
<ol>
<li><strong>AI 推理研究民主化</strong>：讓進階推理技術不再侷限於資金充裕的實驗室</li>
<li><strong>提供可複現基線</strong>：避免重複浪費計算資源</li>
<li><strong>領域特化應用</strong>：允許針對特定推理任務微調</li>
<li><strong>加速創新</strong>：在共享開源基礎上建構，而非依賴專有模型</li>
</ol>
<h3 id="grpo-對-ai-研究的深遠影響">GRPO 對 AI 研究的深遠影響</h3>
<p>GRPO 被 2025-2026 年 AI 社群視為研究亮點：</p>
<ul>
<li>概念有趣且實驗成本合理</li>
<li>已有多個推理模型在數學競賽達到金牌水準</li>
<li>推動「強化學習 + 語言模型」成為主流研究方向</li>
</ul>
<hr>
<h2 id="對-daily-digest-prompt-專案的啟發">對 Daily-Digest-Prompt 專案的啟發</h2>
<ol>
<li><strong>GRPO 理念的應用</strong>：可考慮在 Agent 行為優化中引入類似的「組內相對評估」機制</li>
<li><strong>蒸餾技術</strong>：Mixture-of-Thoughts 的策劃方法可啟發知識庫筆記的品質分層</li>
<li><strong>多階段訓練模式</strong>：SFT → RL → Refinement 的漸進式訓練思路，類似專案中的迭代式品質提升</li>
<li><strong>開源協作模式</strong>：Open R1 的社群協作方式值得參考（GitHub + HF Hub + 部落格更新）</li>
</ol>
<hr>
<h2 id="參考來源">參考來源</h2>
<ol>
<li><a href="https://github.com/huggingface/open-r1">Open R1 GitHub Repository</a></li>
<li><a href="https://huggingface.co/blog/open-r1">Open-R1: A Fully Open Reproduction of DeepSeek-R1 - Hugging Face Blog</a></li>
<li><a href="https://huggingface.co/open-r1">Hugging Face Open R1 Organization</a></li>
<li><a href="https://arxiv.org/pdf/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL - arXiv</a></li>
<li><a href="https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training">The State of Reinforcement Learning for LLM Reasoning - Sebastian Raschka</a></li>
<li><a href="https://techcrunch.com/2025/01/28/hugging-face-researchers-are-trying-to-build-a-more-open-version-of-deepseeks-ai-reasoning-model/">Hugging Face Researchers Build Open Version of DeepSeek-R1 - TechCrunch</a></li>
</ol>
<hr>
<p><em>研究日期：2026-02-17</em>
<em>研究者：Claude Code Agent</em></p>

      </div>

      <a href="../" class="back-link">&larr; 返回首頁</a>
    </article>
  </main>

  <button class="back-to-top" id="backToTop" onclick="window.scrollTo({top:0,behavior:'smooth'})" aria-label="回到頂部">&uarr;</button>

  <footer>
    <div class="container">
      <p>Powered by RAG Knowledge Base</p>
    </div>
  </footer>
  <script>
    function toggleTheme(){const b=document.body;b.classList.toggle('dark');localStorage.setItem('theme',b.classList.contains('dark')?'dark':'light')}
    if(localStorage.getItem('theme')==='dark')document.body.classList.add('dark');
    window.addEventListener('scroll',function(){
      const prog=document.getElementById('readingProgress');
      const btn=document.getElementById('backToTop');
      const h=document.documentElement.scrollHeight-window.innerHeight;
      const pct=h>0?(window.scrollY/h)*100:0;
      prog.style.width=pct+'%';
      btn.classList.toggle('visible',window.scrollY>300);
    });
  </script>
</body>
</html>
